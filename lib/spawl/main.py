#!/usr/bin/python3
import asyncio
import re
import os
import sys 
import argparse
from pyppeteer import launch

DESCRIPTION = "SPAWL : Single Page Application Wordlist generator"

browsed_urls = []
urls_to_browse = []
accumulated_words = {}

def parse_args():
    parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--output", help="File to save the resulting wordlist in", default="spawl.out")
    parser.add_argument("--delay", help="Delay between each page navigation", default=2000)
    parser.add_argument("--headless", help="Spawn or not the browser", action=argparse.BooleanOptionalAction, default=True)
    parser.add_argument("url", help="Url of the website to extract the wordlist")
    return dict(vars(parser.parse_args()))

def extract_domain_from_url(url):
    domain = re.sub(r'^(https?://)?(www\.)?', '', url)
    domain = domain.split("/")[0]
    return domain

def get_next_url_to_browse(domain):
    return next((a for a in urls_to_browse if a not in browsed_urls and domain in a), None)

async def navigate_to(page, url, delay):
    await page.goto(url)
    await asyncio.sleep(delay / 1000)

async def extract_words(page):
    # Get the text content of the page
    page_text = await page.evaluate('document.body.innerText') or ""

    # Split the words
    page_words = re.findall(r'\b\w+\b', page_text)

    # Count occurrences of each word
    for word in page_words:
        accumulated_words[word] = accumulated_words.get(word, 0) + 1

async def extract_links(page):
    links_set = set()
    anchor_elements = await page.querySelectorAll("a")
    for element in anchor_elements:
        href = await page.evaluate('(element) => element.href', element)
        if href:
            links_set.add(href)
    urls_to_browse.extend(links_set)

async def run(url, output, delay, headless):
    # Launch the browser and open a new blank page
    browser = await launch(headless=headless)
    page = await browser.newPage()
    await page.setViewport({'width': 1080, 'height': 1024})

    currentURL = url
    domain = extract_domain_from_url(url)

    while currentURL is not None:
        print("browsing ", currentURL)
        await navigate_to(page, currentURL, delay)
        await extract_words(page)
        await extract_links(page)
        browsed_urls.append(currentURL)
        currentURL = get_next_url_to_browse(domain)

    # Sort words by occurrences in decreasing order
    sorted_words = sorted(accumulated_words.items(), key=lambda x: x[1], reverse=True)
    flat_list = "\n".join(word[0] for word in sorted_words)

    with open(output, 'w') as file:
        file.write(flat_list)
        print("Saved to", output)

    await browser.close()


if __name__ == "__main__":
    args = parse_args()
    asyncio.run(run(**args))


